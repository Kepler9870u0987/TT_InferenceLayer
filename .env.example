# ========================================
# LLM Inference Layer - Configuration
# ========================================
# Copy this file to .env and customize values for your environment.
# DO NOT commit .env to version control!

# === Application ===
APP_NAME="LLM Inference Layer"
APP_VERSION="0.1.0"
DEBUG=false
LOG_LEVEL=INFO

# === Ollama Configuration ===
# Base URL for Ollama API (use http://localhost:11434 for local dev, http://ollama:11434 in Docker)
OLLAMA_BASE_URL=http://ollama:11434

# Model to use for inference (pull with: ollama pull <model>)
# Recommended models:
# - qwen2.5:7b (good structured output, Italian support)
# - llama3.1:8b (good tool calling)
# - mistral:7b (fast, good JSON)
OLLAMA_MODEL=qwen2.5:7b

# Timeout for LLM calls (seconds)
OLLAMA_TIMEOUT=60

# Fallback models (comma-separated, optional)
# Example: FALLBACK_MODELS=llama3.1:8b,mistral:7b
FALLBACK_MODELS=

# === LLM Generation Parameters ===
LLM_TEMPERATURE=0.1  # Low for determinism
LLM_MAX_TOKENS=2048
LLM_STREAM=false  # Always false for easier validation

# === Input Processing ===
# Maximum body length to send to LLM (chars)
BODY_TRUNCATION_LIMIT=8000

# Number of top candidate keywords to include in prompt
CANDIDATE_TOP_N=100

# === Retry & Fallback Configuration ===
# Maximum retry attempts before fallback
MAX_RETRIES=3

# Exponential backoff multiplier for retries
RETRY_BACKOFF_BASE=2.0

# Reduced limits for retry with smaller request
SHRINK_TOP_N=50
SHRINK_BODY_LIMIT=4000

# === PII Redaction ===
# Redact PII before sending to LLM (set to true if using external LLM)
REDACT_FOR_LLM=false

# Always redact for GDPR-compliant storage
REDACT_FOR_STORAGE=true

# PII types to redact (comma-separated)
# Available: CF, PHONE_IT, EMAIL, NAME, ORG
REDACT_PII_TYPES=CF,PHONE_IT,EMAIL,NAME

# === Database Configuration ===
# PostgreSQL connection URL
DATABASE_URL=postgresql+asyncpg://llm_user:llm_pass@postgres:5432/llm_inference

# Connection pool settings
DB_POOL_SIZE=20
DB_MAX_OVERFLOW=10

# === Redis & Celery Configuration ===
# Redis URL (used for both Celery broker and general caching)
REDIS_URL=redis://redis:6379/0

# Celery broker and result backend
CELERY_BROKER_URL=redis://redis:6379/0
CELERY_RESULT_BACKEND=redis://redis:6379/1

# Task time limit (seconds)
CELERY_TASK_TIME_LIMIT=300

# Worker concurrency
CELERY_WORKER_CONCURRENCY=4

# === Validation Configuration ===
# Path to JSON Schema file (relative to project root)
JSON_SCHEMA_PATH=config/schema/email_triage_v2.json

# Minimum confidence threshold for quality warnings
MIN_CONFIDENCE_WARNING_THRESHOLD=0.2

# Enable/disable verifiers
ENABLE_EVIDENCE_PRESENCE_CHECK=true
ENABLE_KEYWORD_PRESENCE_CHECK=true

# === Monitoring ===
PROMETHEUS_ENABLED=true
METRICS_PORT=9090

# === Pipeline Versioning ===
# Dictionary version (frozen during batch processing)
DICTIONARY_VERSION=1

# Inference layer version
INFERENCE_LAYER_VERSION=0.1.0

# Schema version identifier
SCHEMA_VERSION=email_triage_v2

# === Feature Flags ===
# Enable async API endpoints (Celery-based)
ENABLE_ASYNC_API=true

# Enable batch processing endpoints
ENABLE_BATCH_API=true
