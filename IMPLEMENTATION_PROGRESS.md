# LLM Inference Layer â€” Implementation Progress Tracker

> **Progetto**: Thread Classificator Mail - LLM Inference Layer  
> **Data inizio**: 2026-02-19  
> **Ultimo aggiornamento**: 2026-02-19  
> **Stato generale**: ðŸŸ¡ IN PROGRESS (Fase 0, 1, 2 completate - Fase 3 prossima)

---

## Quick Status Overview

| Fase | Stato | Completamento | Note |
|------|-------|---------------|------|
| **Fase 0** â€” Scaffolding | ðŸŸ¢ Completed | 100% | Structure, pyproject.toml, Docker, README done |
| **Fase 1** â€” Data Models | ðŸŸ¢ Completed | 100% | Enums, input/output models, JSON Schema, fixtures done |
| **Fase 2** â€” LLM Client | ðŸŸ¢ Completed | 100% | BaseLLMClient, OllamaClient, PromptBuilder, PII redactor, tests done |
| **Fase 3** â€” Validation | âšª Not Started | 0% | - |
| **Fase 4** â€” Retry Engine | âšª Not Started | 0% | - |
| **Fase 5** â€” API FastAPI | âšª Not Started | 0% | - |
| **Fase 6** â€” PII Redaction | âšª Not Started | 0% | - |
| **Fase 7** â€” Persistenza | âšª Not Started | 0% | - |
| **Fase 8** â€” Config & Docker | âšª Not Started | 0% | - |
| **Fase 9** â€” Tests | âšª Not Started | 0% | - |
| **Fase 10** â€” Logging & CI | âšª Not Started | 0% | - |

**Legenda**: ðŸŸ¢ Completed | ðŸŸ¡ In Progress | âšª Not Started | ðŸ”´ Blocked

---

## Fase 0 â€” Scaffolding Progetto (1â€“2 giorni) âœ… COMPLETED

### Tasks

- [x] 0.1 â€” Creare struttura directory completa (src/, tests/, config/, docker/)
- [x] 0.2 â€” pyproject.toml con dipendenze base
- [x] 0.3 â€” docker-compose.yml (api, ollama, redis, postgres, worker)
- [x] 0.4 â€” .env.example con tutte le variabili di config
- [x] 0.5 â€” README.md con setup e architettura

### Files Created
- `src/inference_layer/__init__.py`
- `src/inference_layer/main.py`
- `src/inference_layer/config.py`
- `src/inference_layer/models/__init__.py`
- `src/inference_layer/api/__init__.py`
- `src/inference_layer/llm/__init__.py`
- `src/inference_layer/validation/__init__.py`
- `src/inference_layer/retry/__init__.py`
- `src/inference_layer/pii/__init__.py`
- `src/inference_layer/tasks/__init__.py`
- `src/inference_layer/persistence/__init__.py`
- `tests/unit/__init__.py`
- `tests/integration/__init__.py`
- `tests/fixtures/__init__.py`
- `pyproject.toml`
- `docker-compose.yml`
- `docker/Dockerfile`
- `docker/Dockerfile.worker`
- `.env.example`
- `README.md`

### Notes
- Decisioni: Python 3.11, FastAPI, Pydantic v2, Docker Compose completo
- Candidate keywords arrivano dall'upstream (non generati qui)
- PII NON redattati in input; redaction on-the-fly solo per LLM esterni
- API sia sincrona che asincrona (Celery)

---

## Fase 1 â€” Data Models (Pydantic v2) (2â€“3 giorni) âœ… COMPLETED

### Tasks

- [x] 1.1 â€” Enums (TopicsEnum, SentimentEnum, PriorityEnum)
- [x] 1.2 â€” Input models (PiiEntity, RemovedSection, EmailDocument, CandidateKeyword, TriageRequest)
- [x] 1.3 â€” Output models (KeywordInText, EvidenceItem, TopicResult, SentimentResult, PriorityResult, EmailTriageResponse, TriageResult)
- [x] 1.4 â€” PipelineVersion (frozen dataclass)
- [x] 1.5 â€” JSON Schema email_triage_v2.json
- [x] 1.6 â€” Sample fixtures (email, candidates, valid response)

### Files Created
- `src/inference_layer/models/enums.py`
- `src/inference_layer/models/pipeline_version.py`
- `src/inference_layer/models/input_models.py`
- `src/inference_layer/models/output_models.py`
- `config/schema/email_triage_v2.json`
- `tests/fixtures/sample_email.json`
- `tests/fixtures/sample_candidates.json`
- `tests/fixtures/valid_llm_response.json`

### Notes
- Schema strict: additionalProperties=false, min/max vincoli
- ConformitÃ  al JSON Schema email_triage_v2

---

## Fase 2 â€” LLM Client Abstraction + Prompt Builder (3â€“4 giorni)

### Tasks
---

## Fase 2 â€” LLM Client Abstraction + Prompt Builder (3â€“4 giorni) âœ… COMPLETED

### Tasks

- [x] 2.1 â€” Abstract base client (BaseLLMClient ABC)
- [x] 2.2 â€” Ollama client implementation (structured output JSON)
- [x] 2.3 â€” SGLang client stub (per futuro)
- [x] 2.4 â€” Prompt builder (system + user payload, truncation, top-N)
- [x] 2.5 â€” Text utilities (truncation, PII span adjustment)
- [x] 2.6 â€” PII redactor (on-the-fly redaction)
- [x] 2.7 â€” LLM-specific models (LLMGenerationRequest, LLMGenerationResponse, LLMMetadata)
- [x] 2.8 â€” LLM exceptions hierarchy
- [x] 2.9 â€” Prompt templates (Jinja2)
- [x] 2.10 â€” Unit tests (text_utils, redactor, prompt_builder)
- [x] 2.11 â€” Integration tests (Ollama client)
- [x] 2.12 â€” Update config with LLM settings
- [x] 2.13 â€” Update module exports

### Files Created
- `src/inference_layer/models/llm_models.py` (LLMGenerationRequest, LLMGenerationResponse, LLMMetadata)
- `src/inference_layer/llm/exceptions.py` (LLM exception hierarchy)
- `src/inference_layer/llm/base_client.py` (BaseLLMClient ABC)
- `src/inference_layer/llm/ollama_client.py` (OllamaClient with httpx AsyncClient)
- `src/inference_layer/llm/sglang_client.py` (SGLangClient stub)
- `src/inference_layer/llm/text_utils.py` (truncate_at_sentence_boundary, adjust_pii_spans, count_tokens_approximate)
- `src/inference_layer/llm/prompt_builder.py` (PromptBuilder with Jinja2)
- `src/inference_layer/pii/redactor.py` (redact_pii_for_llm, redact_pii_in_candidates)
- `config/prompts/system_prompt.txt` (System prompt template)
- `config/prompts/user_prompt_template.txt` (User prompt template)
- `tests/unit/llm/test_text_utils.py` (Unit tests for text utilities)
- `tests/unit/llm/test_prompt_builder.py` (Unit tests for prompt builder)
- `tests/unit/pii/test_redactor.py` (Unit tests for PII redaction)
- `tests/integration/llm/test_ollama_integration.py` (Integration tests for Ollama)

### Notes
- **Architecture**: Model-agnostic abstraction with BaseLLMClient ABC
- **Ollama Client**: Async implementation using httpx.AsyncClient with connection pooling
- **Structured Output**: JSON Schema passed via `format` parameter to Ollama
- **Retry Logic**: Built-in connection-level retries (2 attempts) with exponential backoff
- **Prompt Engineering**: Jinja2 templates for maintainability and version control
- **Text Processing**: Sentence-boundary truncation (8000 chars normal, 4000 shrink)
- **Candidate Selection**: Top-N filtering (100 normal, 50 shrink)
- **PII Handling**: On-the-fly redaction (configurable, default OFF for self-hosted Ollama)
- **Temperature**: 0.1 for determinism
- **Testing**: Unit tests for all utilities, integration tests for Ollama (requires running server)

### Decisions Made
- **httpx over ollama package**: Direct HTTP control, no extra dependencies
- **Async-only**: Consistent with FastAPI, better scalability
- **Jinja2 templates**: Prompts as external files for maintainability
- **Sentence boundary truncation**: Preserves semantic coherence over simple char truncation
- **PII redaction configurable**: OFF by default (safe for self-hosted), ready for external LLMs

---

## Fase 3 â€” Validazione Multi-Stadio (3â€“4 giorni)

### Tasks

- [ ] 3.1 â€” Stage 1: JSON Parse
- [ ] 3.2 â€” Stage 2: JSON Schema validation
- [ ] 3.3 â€” Stage 3: Business rules (candidateid exists, labelid in enum)
- [ ] 3.4 â€” Stage 4: Quality checks (confidence gating, dedup, warnings)
- [ ] 3.5 â€” Verifiers extra (evidence presence, keyword presence, spans coherence)
- [ ] 3.6 â€” Pipeline orchestrator

### Files Created
- N/A

### Notes
- Hard fail su stage 1/2/3 â†’ retry
- Warnings su stage 4 â†’ salvati ma non bloccanti

---

## Fase 4 â€” Retry Engine + Fallback (2â€“3 giorni)

### Tasks

- [ ] 4.1 â€” Retry standard (max 3 tentativi, backoff esponenziale)
- [ ] 4.2 â€” Shrink request (meno candidati + body piÃ¹ corto)
- [ ] 4.3 â€” Fallback modello alternativo
- [ ] 4.4 â€” DLQ routing + logging

### Files Created
- N/A

### Notes
- Policy a 4 livelli: retry â†’ shrink â†’ fallback â†’ DLQ
- Logging strutturato per audit

---

## Fase 5 â€” API FastAPI (2â€“3 giorni)

### Tasks

- [ ] 5.1 â€” Endpoint sincrono POST /triage
- [ ] 5.2 â€” Endpoint asincrono POST /triage/batch
- [ ] 5.3 â€” Endpoint GET /triage/task/{task_id}
- [ ] 5.4 â€” Health check GET /health
- [ ] 5.5 â€” Schema endpoint GET /schema
- [ ] 5.6 â€” Celery tasks (triage_email, triage_batch)
- [ ] 5.7 â€” Celery app configuration

### Files Created
- N/A

### Notes
- Sincrono per singola email (demo/test rapidi)
- Asincrono per batch (produzione)

---

## Fase 6 â€” PII Redaction on-the-fly (1â€“2 giorni)

### Tasks

- [ ] 6.1 â€” Redactor module (redact_text basato su pii_entities annotate)
- [ ] 6.2 â€” Redaction per LLM esterni (configurabile)
- [ ] 6.3 â€” Redaction per persistenza GDPR

### Files Created
- N/A

### Notes
- Input body NON redattato
- Redaction applicata on-the-fly solo quando necessario (LLM esterno / storage)

---

## Fase 7 â€” Persistenza (2 giorni)

### Tasks

- [ ] 7.1 â€” Database schema (triage_results, dlq_triage_failures)
- [ ] 7.2 â€” Repository pattern (save_result, save_to_dlq, get_result)
- [ ] 7.3 â€” SQLAlchemy async setup

### Files Created
- N/A

### Notes
- PostgreSQL con JSONB per response e pipeline_version
- DLQ table per failure tracking

---

## Fase 8 â€” Configurazione e Docker (2 giorni)

### Tasks

- [ ] 8.1 â€” Settings module (Pydantic BaseSettings)
- [ ] 8.2 â€” Dockerfile (multi-stage build)
- [ ] 8.3 â€” Dockerfile.worker
- [ ] 8.4 â€” docker-compose.yml completo con healthchecks

### Files Created
- N/A

### Notes
- Tutte le variabili configurabili via env
- GPU passthrough per Ollama

---

## Fase 9 â€” Tests (3â€“4 giorni)

### Tasks

- [ ] 9.1 â€” Unit tests (models, prompt builder, validators, verifiers, retry)
- [ ] 9.2 â€” Integration tests (Ollama client, full pipeline, API)
- [ ] 9.3 â€” Fixtures (sample email, candidates, valid/invalid responses)

### Files Created
- N/A

### Notes
- Target: copertura â‰¥ 85%
- Integration tests con Ollama in CI se possibile

---

## Fase 10 â€” Logging, Metriche, CI (2â€“3 giorni)

### Tasks

- [ ] 10.1 â€” Structured logging con structlog
- [ ] 10.2 â€” Metriche Prometheus custom
- [ ] 10.3 â€” CI pipeline (lint, type check, tests, build)

### Files Created
- N/A

### Notes
- Metriche chiave: validation_failures, retries, dlq_entries, unknown_topic_ratio

---

## Known Issues / Blockers

_Nessun blocker al momento._

---

## Next Steps (Current Sprint)

1. âœ… Creare file di tracking
2. âœ… Completare scaffolding base (directory structure)
3. âœ… pyproject.toml con dipendenze
4. âœ… docker-compose.yml
5. âœ… .env.example e README.md
6. âœ… Implementare data models (enums, input models, output models)
7. âœ… Implementare LLM client abstraction (BaseLLMClient, OllamaClient)
8. âœ… Implementare prompt builder (Jinja2 templates, truncation, top-N)
9. âœ… Implementare PII redactor e text utilities
10. âœ… Unit & integration tests per Fase 2
11. ðŸ”„ **CURRENT**: Implementare validation pipeline (4 stages + verifiers)
12. ðŸ”œ Implementare retry engine con fallback strategies
13. ðŸ”œ Implementare API FastAPI (endpoints sincroni/asincroni)

---

## Decision Log

| Data | Decisione | Rationale |
|------|-----------|-----------|
| 2026-02-19 | Scope: solo layer LLM (no candidate keyword generator) | Candidate keywords arrivano dall'upstream |
| 2026-02-19 | PII: body NON redattato in input | Permette analisi LLM piÃ¹ ricca; redaction on-the-fly solo per LLM esterni/storage |
| 2026-02-19 | API: sincrona + asincrona (Celery) | Sincrona per demo, asincrona per batch produzione |
| 2026-02-19 | Stack: Python 3.11, FastAPI, Pydantic v2, Docker Compose | Coerente con design doc v2/v3 |
| 2026-02-19 | Model: astrazione model-agnostic | Facilita switch Ollama â†’ SGLang in futuro |
| 2026-02-19 | LLM Client: httpx diretto (no ollama package) | Maggiore controllo, no dipendenze extra, facilita debugging |
| 2026-02-19 | Async-only per LLM client | Coerenza con FastAPI async; migliore scalabilitÃ  |
| 2026-02-19 | Prompts: Jinja2 templates in config/prompts/ | ManutenibilitÃ , versionamento, sperimentazione facilitata |
| 2026-02-19 | Truncation: sentence boundary | Preserva contesto semantico vs hard truncation |
| 2026-02-19 | Jinja2 dependency added | Per prompt templating (3.1.0+)

---

## References

- [Design Doc](doc/LLM_Layer_Consolidato_v2v3_chat_SUPER_DETAILED.md)
- [JSON Schema](config/schema/email_triage_v2.json) â€” da creare
- [Sample Input](tests/fixtures/sample_email.json) â€” da creare

---

**Istruzioni per riprendere dopo interruzione**:
1. Leggere questo file per vedere lo stato attuale
2. Controllare l'ultima sezione "Next Steps" per i task correnti
3. Verificare "Known Issues / Blockers" per problemi aperti
4. Controllare "Decision Log" per contesto decisionale
5. Riprendere dall'ultima task non completata
